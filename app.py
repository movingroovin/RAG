import os
import shutil
import gradio as gr
import requests
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnablePassthrough

# è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹ (FastEmbed)
embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-zh-v1.5")

# åˆå§‹åŒ–å‘é‡è³‡æ–™åº« è·¯å¾‘
DB_PATH = "./chroma_db"
UPLOAD_DIR = "./upload"
os.makedirs(UPLOAD_DIR, exist_ok=True)

vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

def list_indexed_files():
    """
    å¾å‘é‡è³‡æ–™åº«ä¸­ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨
    """
    try:
        data = vectorstore.get()
        if not data or not data['metadatas']:
            return "ç›®å‰çŸ¥è­˜åº«ç‚ºç©º", []
        
        sources = set()
        for meta in data['metadatas']:
            if 'source' in meta:
                # å„²å­˜å®Œæ•´è·¯å¾‘ä»¥ä¾¿å¾ŒçºŒåˆªé™¤ï¼Œä½†é¡¯ç¤ºæ™‚åªé¡¯ç¤ºæª”å
                sources.add(meta['source'])
        
        if not sources:
            return "ç›®å‰çŸ¥è­˜åº«ä¸­ç„¡æ–‡ä»¶ä¾†æº", []
        
        # å»ºç«‹é¡¯ç¤ºæ–‡å­—
        display_text = "\n".join([f"ğŸ“„ {os.path.basename(s)}" for s in sorted(list(sources))])
        # å›å‚³é¡¯ç¤ºæ–‡å­—èˆ‡åŸå§‹è·¯å¾‘æ¸…å–®ï¼ˆç”¨æ–¼ä¸‹æ‹‰é¸å–®ï¼‰
        return display_text, sorted(list(sources))
    except Exception as e:
        return f"ç„¡æ³•è®€å–æ¸…å–®: {str(e)}", []

def delete_file(file_path):
    """
    å¾å‘é‡è³‡æ–™åº«ä¸­åˆªé™¤æŒ‡å®šæ–‡ä»¶
    """
    if not file_path:
        return "è«‹å…ˆé¸æ“‡è¦åˆªé™¤çš„æ–‡ä»¶", *list_indexed_files()
    
    try:
        # Chroma å¯ä»¥é€é metadata é€²è¡Œéæ¿¾åˆªé™¤
        vectorstore.delete(where={"source": file_path})
        
        # åŒæ™‚åˆªé™¤æœ¬åœ° upload è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
        if os.path.exists(file_path):
            os.remove(file_path)
        
        filename = os.path.basename(file_path)
        status = f"å·²æˆåŠŸå¾çŸ¥è­˜åº«èˆ‡è³‡æ–™å¤¾ä¸­åˆªé™¤æ–‡ä»¶ï¼š{filename}"
        
        # ç²å–æ›´æ–°å¾Œçš„æ¸…å–®
        display_text, file_list = list_indexed_files()
        return status, display_text, gr.update(choices=file_list, value=None)
    except Exception as e:
        return f"åˆªé™¤å¤±æ•—: {str(e)}", *list_indexed_files()

def get_groq_models():
    """
    å¾ Groq API ç²å–å¯ç”¨æ¨¡å‹æ¸…å–®
    """
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        return ["openai/gpt-oss-120b"] # é è¨­æ¨¡å‹
    
    url = "https://api.groq.com/openai/v1/models"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            models_data = response.json()
            # å–å¾—æ¨¡å‹ ID ä¸¦ä»¥å­—æ¯åºæ’åº
            model_ids = [model['id'] for model in models_data['data']]
            return sorted(model_ids)
        else:
            print(f"ç„¡æ³•ç²å–æ¨¡å‹: {response.status_code}")
            return ["openai/gpt-oss-120b"]
    except Exception as e:
        print(f"ç²å–æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return ["openai/gpt-oss-120b"]

# è¨˜æ†¶é«”å­˜å„²
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

def process_files(files):
    """
    è™•ç†ä¸Šå‚³çš„æ–‡ä»¶ï¼Œå„²å­˜è‡³ upload è³‡æ–™å¤¾ï¼Œä¸¦åŠ å…¥å‘é‡è³‡æ–™åº«
    """
    if not files:
        return "æœªé¸æ“‡ä»»ä½•æª”æ¡ˆ"
    
    documents = []
    saved_files = []
    
    for file in files:
        # å–å¾—æª”åä¸¦å„²å­˜åˆ°æŒ‡å®šçš„ upload è³‡æ–™å¤¾
        filename = os.path.basename(file.name)
        dest_path = os.path.join(UPLOAD_DIR, filename)
        shutil.copy(file.name, dest_path)
        saved_files.append(dest_path)
        
        file_path = dest_path
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            documents.extend(loader.load())
        elif file_path.endswith('.txt') or file_path.endswith('.md'):
            loader = TextLoader(file_path)
            documents.extend(loader.load())
    
    if not documents:
        return "æ²’æœ‰æ‰¾åˆ°å¯è®€å–çš„å…§å®¹"

    # åˆ‡åˆ†æ–‡æœ¬
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)
    print(f"åˆ‡åˆ†ç‚º {len(splits)} å€‹å€å¡Š")
    
    # åŠ å…¥å‘é‡è³‡æ–™åº«
    vectorstore.add_documents(documents=splits)
    
    display_text, file_list = list_indexed_files()
    return f"æˆåŠŸè™•ç† {len(files)} å€‹æª”æ¡ˆï¼Œåˆ‡åˆ†ç‚º {len(splits)} å€‹å€å¡Šä¸¦å·²åŠ å…¥çŸ¥è­˜åº«ã€‚", display_text, gr.update(choices=file_list)

def chat_response(message, history, model_name, use_rag):
    """
    è™•ç†ç”¨æˆ¶è¨Šæ¯ä¸¦è¿”å› AI å›æ‡‰
    """
    try:
        # ä¾ç…§é¸æ“‡çš„æ¨¡å‹å‹•æ…‹åˆå§‹åŒ– LLM
        llm = ChatGroq(
            model=model_name,
            temperature=0.7,
            max_tokens=1000
        )

        if use_rag:
            # RAG æ¨¡å¼ä¸‹çš„æç¤ºæ¨¡æ¿
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ã€‚è«‹åƒ…æ ¹æ“šä¸‹æ–¹æä¾›çš„ã€ä¸Šä¸‹æ–‡å…§å®¹ã€‘ä¾†å›ç­”å•é¡Œã€‚å¦‚æœå•é¡Œçš„ç­”æ¡ˆä¸åœ¨å…§å®¹ä¸­ï¼Œè«‹ç›´æ¥å›ç­”ï¼šã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„çŸ¥è­˜åº«å…§å®¹ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚ã€ï¼Œä¸è¦å˜—è©¦åˆ©ç”¨æ‚¨åŸæœ‰çš„çŸ¥è­˜ä¾†å›ç­”ã€‚è‹¥å›ç­”æ˜¯ä¸­æ–‡ï¼Œç”¨ç¹é«”ä¸­æ–‡\n\nä¸Šä¸‹æ–‡å…§å®¹ï¼š\n{context}"),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])
            
            # æª¢ç´¢ç›¸é—œå…§å®¹
            docs = retriever.invoke(message)
            context = "\n\n".join([doc.page_content for doc in docs])
            
            # çµ„æˆéˆ
            chain = prompt | llm
            chain_with_history = RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="input",
                history_messages_key="history"
            )
            
            response = chain_with_history.invoke(
                {"input": message, "context": context},
                config={"configurable": {"session_id": "default"}}
            )
        else:
            # æ™®é€šå°è©±æ¨¡å¼
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are a helpful assistant."),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])

            chain = prompt | llm
            chain_with_history = RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="input",
                history_messages_key="history"
            )

            response = chain_with_history.invoke(
                {"input": message},
                config={"configurable": {"session_id": "default"}}
            )
        
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name} | RAG: {use_rag} | ç”¨æˆ¶è¨Šæ¯: {message}")
        
        # Gradio 5.0+ ä½¿ç”¨å­—å…¸æ ¼å¼
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response.content})
        
        return "", history
    except Exception as e:
        error_msg = f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(error_msg)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": error_msg})
        return "", history

def clear_history():
    """
    æ¸…é™¤å°è©±æ­·å²
    """
    if "default" in store:
        store["default"] = InMemoryChatMessageHistory()
    return []

# å–å¾—å¯ç”¨æ¨¡å‹å’Œåˆå§‹æ–‡ä»¶æ¸…å–®
available_models = get_groq_models()
initial_indexed_text, initial_file_list = list_indexed_files()

# è¨­å®šé è¨­æ¨¡å‹é‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨ "openai/gpt-oss-120b"ï¼Œè‹¥ä¸åœ¨æ¸…å–®ä¸­å‰‡é¸ç¬¬ä¸€å€‹
default_model = "openai/gpt-oss-120b"
if available_models and default_model not in available_models:
    default_model = available_models[0]

# å‰µå»º Gradio ä»‹é¢
with gr.Blocks(title="LangChain + Gradio RAG æ‡‰ç”¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– LangChain + Groq RAG æ‡‰ç”¨")
    
    with gr.Sidebar():
        gr.Markdown("## ğŸ“š çŸ¥è­˜åº«è¨­å®š")
        rag_toggle = gr.Checkbox(label="å•Ÿç”¨ RAG åŠŸèƒ½", value=False)
        
        with gr.Tab("ä¸Šå‚³æ–‡ä»¶"):
            file_upload = gr.File(
                label="ä¸Šå‚³æ–°æ–‡ä»¶",
                file_types=[".pdf", ".txt", ".md"],
                file_count="multiple"
            )
            process_btn = gr.Button("æ›´æ–°çŸ¥è­˜åº«", variant="primary")
        
        with gr.Tab("ç®¡ç†æ–‡ä»¶"):
            file_to_delete = gr.Dropdown(
                label="é¸æ“‡è¦åˆªé™¤çš„æ–‡ä»¶",
                choices=initial_file_list,
                interactive=True
            )
            delete_btn = gr.Button("åˆªé™¤é¸å®šæ–‡ä»¶", variant="stop")
            
        upload_status = gr.Textbox(label="è™•ç†ç‹€æ…‹", interactive=False)
        
        gr.Markdown("### ğŸ“‚ ç›®å‰çŸ¥è­˜åº«å…§å®¹")
        indexed_files_display = gr.Markdown(initial_indexed_text)
        
        gr.Markdown("---")
        gr.Markdown("### æ¨¡å‹è¨­å®š")
        model_selector = gr.Dropdown(
            choices=available_models,
            value=default_model,
            label="é¸æ“‡ Groq æ¨¡å‹",
            interactive=True
        )

    with gr.Column():
        chatbot = gr.Chatbot(
            height=500,
            show_label=False,
            container=True,
        )

        with gr.Row():
            msg = gr.Textbox(
                placeholder="è¼¸å…¥æ‚¨çš„è¨Šæ¯...",
                show_label=False,
                container=False,
                scale=7
            )
            submit_btn = gr.Button("é€å‡º", scale=1, variant="primary")
            clear_btn = gr.Button("æ¸…é™¤å°è©±", scale=1)

    gr.Markdown("---")
    gr.Markdown("**ä½¿ç”¨èªªæ˜ï¼š**")
    gr.Markdown("- è‹¥è¦ä½¿ç”¨ RAGï¼Œè«‹å…ˆåœ¨å·¦å´ä¸Šå‚³æ–‡ä»¶ä¸¦é»æ“Šã€Œæ›´æ–°çŸ¥è­˜åº«ã€ï¼Œç„¶å¾Œå‹¾é¸ã€Œå•Ÿç”¨ RAG åŠŸèƒ½ã€")
    gr.Markdown("- åœ¨å·¦å´ä¸‹æ‹‰é¸å–®é¸æ“‡æ‚¨æƒ³ä½¿ç”¨çš„ Groq æ¨¡å‹")
    gr.Markdown("- è«‹ç¢ºä¿å·²è¨­å®šæœ‰æ•ˆçš„ Groq èˆ‡ Google API é‡‘é‘°")

    # è¨­å®šäº‹ä»¶è™•ç†
    process_btn.click(
        process_files, 
        inputs=[file_upload], 
        outputs=[upload_status, indexed_files_display, file_to_delete]
    )

    delete_btn.click(
        delete_file,
        inputs=[file_to_delete],
        outputs=[upload_status, indexed_files_display, file_to_delete]
    )
    
    msg.submit(chat_response, [msg, chatbot, model_selector, rag_toggle], [msg, chatbot])
    submit_btn.click(chat_response, [msg, chatbot, model_selector, rag_toggle], [msg, chatbot])
    clear_btn.click(clear_history, outputs=chatbot)

if __name__ == "__main__":
    # å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )