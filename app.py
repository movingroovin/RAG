import os
import shutil
import gradio as gr
import requests
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnablePassthrough

# è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹ (FastEmbed)
embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-zh-v1.5")

# å°ˆæ¡ˆç®¡ç† è·¯å¾‘
PROJECTS_DIR = "./projects"
os.makedirs(PROJECTS_DIR, exist_ok=True)

def get_project_paths(project_name):
    if not project_name:
        return None, None
    project_path = os.path.join(PROJECTS_DIR, project_name)
    upload_dir = os.path.join(project_path, "upload")
    db_path = os.path.join(project_path, "chroma_db")
    os.makedirs(upload_dir, exist_ok=True)
    os.makedirs(db_path, exist_ok=True)
    return upload_dir, db_path

def list_projects():
    if not os.path.exists(PROJECTS_DIR):
        return []
    return sorted([d for d in os.listdir(PROJECTS_DIR) if os.path.isdir(os.path.join(PROJECTS_DIR, d))])

def list_indexed_files(project_name):
    """
    å¾æŒ‡å®šå°ˆæ¡ˆçš„å‘é‡è³‡æ–™åº«ä¸­ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨
    """
    if not project_name:
        return "è«‹å…ˆé¸æ“‡æˆ–å»ºç«‹å°ˆæ¡ˆ", []
    
    try:
        _, db_path = get_project_paths(project_name)
        vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
        data = vectorstore.get()
        if not data or not data['metadatas']:
            return "ç›®å‰å°ˆæ¡ˆçŸ¥è­˜åº«ç‚ºç©º", []
        
        sources = set()
        for meta in data['metadatas']:
            if 'source' in meta:
                sources.add(meta['source'])
        
        if not sources:
            return "ç›®å‰å°ˆæ¡ˆä¸­ç„¡æ–‡ä»¶ä¾†æº", []
        
        display_text = "\n".join([f"ğŸ“„ {os.path.basename(s)}" for s in sorted(list(sources))])
        return display_text, sorted(list(sources))
    except Exception as e:
        return f"ç„¡æ³•è®€å–æ¸…å–®: {str(e)}", []

def delete_file(file_path, project_name):
    """
    å¾æŒ‡å®šå°ˆæ¡ˆçš„å‘é‡è³‡æ–™åº«ä¸­åˆªé™¤æŒ‡å®šæ–‡ä»¶
    """
    if not project_name:
        return "è«‹å…ˆé¸æ“‡å°ˆæ¡ˆ", "è«‹å…ˆé¸æ“‡å°ˆæ¡ˆ", []
    if not file_path:
        return "è«‹å…ˆé¸æ“‡è¦åˆªé™¤çš„æ–‡ä»¶", *list_indexed_files(project_name)
    
    try:
        _, db_path = get_project_paths(project_name)
        vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
        vectorstore.delete(where={"source": file_path})
        
        if os.path.exists(file_path):
            os.remove(file_path)
        
        filename = os.path.basename(file_path)
        status = f"å·²æˆåŠŸå¾å°ˆæ¡ˆã€Œ{project_name}ã€ä¸­åˆªé™¤æ–‡ä»¶ï¼š{filename}"
        
        display_text, file_list = list_indexed_files(project_name)
        return status, display_text, gr.update(choices=file_list, value=None)
    except Exception as e:
        return f"åˆªé™¤å¤±æ•—: {str(e)}", *list_indexed_files(project_name)

def get_groq_models():
    """
    å¾ Groq API ç²å–å¯ç”¨æ¨¡å‹æ¸…å–®
    """
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        return ["openai/gpt-oss-120b"] # é è¨­æ¨¡å‹
    
    url = "https://api.groq.com/openai/v1/models"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            models_data = response.json()
            # å–å¾—æ¨¡å‹ ID ä¸¦ä»¥å­—æ¯åºæ’åº
            model_ids = [model['id'] for model in models_data['data']]
            return sorted(model_ids)
        else:
            print(f"ç„¡æ³•ç²å–æ¨¡å‹: {response.status_code}")
            return ["openai/gpt-oss-120b"]
    except Exception as e:
        print(f"ç²å–æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return ["openai/gpt-oss-120b"]

# è¨˜æ†¶é«”å­˜å„²
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

def process_files(files, project_name):
    """
    è™•ç†ä¸Šå‚³çš„æ–‡ä»¶ï¼Œå„²å­˜è‡³å°ˆæ¡ˆç›®éŒ„ä¸‹çš„ upload è³‡æ–™å¤¾ï¼Œä¸¦åŠ å…¥è©²å°ˆæ¡ˆçš„å‘é‡è³‡æ–™åº«
    """
    if not project_name:
        return "è«‹å…ˆé¸æ“‡æˆ–å»ºç«‹å°ˆæ¡ˆ", "", gr.update()
    if not files:
        return "æœªé¸æ“‡ä»»ä½•æª”æ¡ˆ", *list_indexed_files(project_name)
    
    upload_dir, db_path = get_project_paths(project_name)
    documents = []
    
    for file in files:
        filename = os.path.basename(file.name)
        dest_path = os.path.join(upload_dir, filename)
        shutil.copy(file.name, dest_path)
        
        if dest_path.endswith('.pdf'):
            loader = PyPDFLoader(dest_path)
            documents.extend(loader.load())
        elif dest_path.endswith('.txt') or dest_path.endswith('.md'):
            loader = TextLoader(dest_path)
            documents.extend(loader.load())
    
    if not documents:
        return "æ²’æœ‰æ‰¾åˆ°å¯è®€å–çš„å…§å®¹", *list_indexed_files(project_name)

    # åˆ‡åˆ†æ–‡æœ¬
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)
    
    # åˆå§‹åŒ–ä¸¦åŠ å…¥è©²å°ˆæ¡ˆçš„å‘é‡è³‡æ–™åº«
    vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
    vectorstore.add_documents(documents=splits)
    
    status = f"æˆåŠŸè™•ç† {len(files)} å€‹æª”æ¡ˆï¼Œä¸¦å·²åŠ å…¥å°ˆæ¡ˆã€Œ{project_name}ã€çŸ¥è­˜åº«ã€‚"
    display_text, file_list = list_indexed_files(project_name)
    return status, display_text, gr.update(choices=file_list)

def chat_response(message, history, model_name, use_rag, project_name):
    """
    è™•ç†ç”¨æˆ¶è¨Šæ¯ä¸¦è¿”å› AI å›æ‡‰
    """
    try:
        if use_rag and not project_name:
            error_msg = "è«‹å…ˆé¸æ“‡å°ˆæ¡ˆä»¥ä½¿ç”¨ RAG åŠŸèƒ½"
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_msg})
            return "", history

        # ä¾ç…§é¸æ“‡çš„æ¨¡å‹å‹•æ…‹åˆå§‹åŒ– LLM
        llm = ChatGroq(
            model=model_name,
            temperature=0.7,
            max_tokens=1000
        )

        if use_rag:
            # RAG æ¨¡å¼ä¸‹çš„æç¤ºæ¨¡æ¿
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ã€‚è«‹åƒ…æ ¹æ“šä¸‹æ–¹æä¾›çš„ã€ä¸Šä¸‹æ–‡å…§å®¹ã€‘ä¾†å›ç­”å•é¡Œã€‚å¦‚æœå•é¡Œçš„ç­”æ¡ˆä¸åœ¨å…§å®¹ä¸­ï¼Œè«‹ç›´æ¥å›ç­”ï¼šã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„çŸ¥è­˜åº«å…§å®¹ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚ã€ï¼Œä¸è¦å˜—è©¦åˆ©ç”¨æ‚¨åŸæœ‰çš„çŸ¥è­˜ä¾†å›ç­”ã€‚è‹¥å›ç­”æ˜¯ä¸­æ–‡ï¼Œç”¨ç¹é«”ä¸­æ–‡\n\nä¸Šä¸‹æ–‡å…§å®¹ï¼š\n{context}"),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])
            
            # å»ºç«‹é‡å°è©²å°ˆæ¡ˆçš„æª¢ç´¢å™¨
            _, db_path = get_project_paths(project_name)
            vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            
            # æª¢ç´¢ç›¸é—œå…§å®¹
            docs = retriever.invoke(message)
            context = "\n\n".join([doc.page_content for doc in docs])
            
            # çµ„æˆéˆ
            chain = prompt | llm
            chain_with_history = RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="input",
                history_messages_key="history"
            )
            
            response = chain_with_history.invoke(
                {"input": message, "context": context},
                config={"configurable": {"session_id": "default"}}
            )
        else:
            # æ™®é€šå°è©±æ¨¡å¼
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are a helpful assistant."),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])

            chain = prompt | llm
            chain_with_history = RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="input",
                history_messages_key="history"
            )

            response = chain_with_history.invoke(
                {"input": message},
                config={"configurable": {"session_id": "default"}}
            )
        
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name} | RAG: {use_rag} | ç”¨æˆ¶è¨Šæ¯: {message}")
        
        # Gradio 5.0+ ä½¿ç”¨å­—å…¸æ ¼å¼
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response.content})
        
        return "", history
    except Exception as e:
        error_msg = f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(error_msg)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": error_msg})
        return "", history

def clear_history():
    """
    æ¸…é™¤å°è©±æ­·å²
    """
    if "default" in store:
        store["default"] = InMemoryChatMessageHistory()
    return []

def create_project(new_name):
    if not new_name:
        return "å°ˆæ¡ˆåç¨±ä¸èƒ½ç‚ºç©º", gr.update(), ""
    
    existing_projects = list_projects()
    if new_name in existing_projects:
        return f"å°ˆæ¡ˆã€Œ{new_name}ã€å·²å­˜åœ¨", gr.update(), ""
    
    # å»ºç«‹å°ˆæ¡ˆç›®éŒ„
    get_project_paths(new_name)
    updated_projects = list_projects()
    return f"æˆåŠŸå»ºç«‹å°ˆæ¡ˆï¼š{new_name}", gr.update(choices=updated_projects, value=new_name), ""

def on_project_change(project_name):
    display_text, file_list = list_indexed_files(project_name)
    return display_text, gr.update(choices=file_list, value=None), f"å·²åˆ‡æ›è‡³å°ˆæ¡ˆï¼š{project_name}" if project_name else "è«‹é¸æ“‡å°ˆæ¡ˆ"

# å–å¾—å¯ç”¨æ¨¡å‹å’Œåˆå§‹å°ˆæ¡ˆæ¸…å–®
available_models = get_groq_models()
available_projects = list_projects()

# è¨­å®šé è¨­æ¨¡å‹é‚è¼¯
default_model = "openai/gpt-oss-120b"
if available_models and default_model not in available_models:
    default_model = available_models[0]

# å‰µå»º Gradio ä»‹é¢
with gr.Blocks(title="LangChain + Gradio RAG æ‡‰ç”¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– LangChain + Groq å¤šå°ˆæ¡ˆ RAG æ‡‰ç”¨")
    
    with gr.Sidebar():
        gr.Markdown("## ğŸ“ å°ˆæ¡ˆç®¡ç†")
        project_selector = gr.Dropdown(
            label="é¸æ“‡ç¾æœ‰å°ˆæ¡ˆ",
            choices=available_projects,
            value=available_projects[0] if available_projects else None,
            interactive=True
        )
        
        with gr.Row():
            new_project_input = gr.Textbox(label="æ–°å°ˆæ¡ˆåç¨±", placeholder="è¼¸å…¥å°ˆæ¡ˆåç¨±...", scale=2)
            create_project_btn = gr.Button("æ–°å¢", scale=1)
        
        gr.Markdown("---")
        gr.Markdown("## ğŸ“š çŸ¥è­˜åº«è¨­å®š")
        rag_toggle = gr.Checkbox(label="å•Ÿç”¨ RAG åŠŸèƒ½", value=False)
        
        with gr.Tab("ä¸Šå‚³æ–‡ä»¶"):
            file_upload = gr.File(
                label="ä¸Šå‚³æ–°æ–‡ä»¶",
                file_types=[".pdf", ".txt", ".md"],
                file_count="multiple"
            )
            process_btn = gr.Button("æ›´æ–°çŸ¥è­˜åº«", variant="primary")
        
        with gr.Tab("ç®¡ç†æ–‡ä»¶"):
            file_to_delete = gr.Dropdown(
                label="é¸æ“‡è¦åˆªé™¤çš„æ–‡ä»¶",
                choices=[],
                interactive=True
            )
            delete_btn = gr.Button("åˆªé™¤é¸å®šæ–‡ä»¶", variant="stop")
            
        upload_status = gr.Textbox(label="ç‹€æ…‹è¨Šæ¯", interactive=False)
        
        gr.Markdown("### ğŸ“‚ ç›®å‰å°ˆæ¡ˆæ–‡ä»¶")
        indexed_files_display = gr.Markdown("è«‹å…ˆé¸æ“‡å°ˆæ¡ˆ")
        
        gr.Markdown("---")
        gr.Markdown("### æ¨¡å‹è¨­å®š")
        model_selector = gr.Dropdown(
            choices=available_models,
            value=default_model,
            label="é¸æ“‡ Groq æ¨¡å‹",
            interactive=True
        )

    with gr.Column():
        chatbot = gr.Chatbot(
            height=500,
            show_label=False,
            container=True,
        )

        with gr.Row():
            msg = gr.Textbox(
                placeholder="è¼¸å…¥æ‚¨çš„è¨Šæ¯...",
                show_label=False,
                container=False,
                scale=7
            )
            submit_btn = gr.Button("é€å‡º", scale=1, variant="primary")
            clear_btn = gr.Button("æ¸…é™¤å°è©±", scale=1)

    gr.Markdown("---")
    gr.Markdown("**ä½¿ç”¨èªªæ˜ï¼š**")
    gr.Markdown("1. **é¸æ“‡å°ˆæ¡ˆ**ï¼šå¾å·¦å´ä¸‹æ‹‰é¸å–®é¸æ“‡ç¾æœ‰å°ˆæ¡ˆï¼Œæˆ–è¼¸å…¥åç¨±ä¸¦é»é¸ã€Œæ–°å¢ã€ä¾†å»ºç«‹æ–°å°ˆæ¡ˆã€‚")
    gr.Markdown("2. **ä¸Šå‚³æ–‡ä»¶**ï¼šåœ¨è©²å°ˆæ¡ˆä¸‹ä¸Šå‚³ PDF/TXT/MD æª”ï¼Œä¸¦é»æ“Šã€Œæ›´æ–°çŸ¥è­˜åº«ã€ã€‚")
    gr.Markdown("3. **é–‹å•Ÿ RAG**ï¼šå‹¾é¸ã€Œå•Ÿç”¨ RAG åŠŸèƒ½ã€å³å¯é–‹å§‹é‡å°è©²å°ˆæ¡ˆå…§å®¹é€²è¡Œå•ç­”ã€‚")

    # åˆå§‹åŒ–é¡¯ç¤º
    demo.load(
        on_project_change,
        inputs=[project_selector],
        outputs=[indexed_files_display, file_to_delete, upload_status]
    )

    # è¨­å®šäº‹ä»¶è™•ç†
    project_selector.change(
        on_project_change,
        inputs=[project_selector],
        outputs=[indexed_files_display, file_to_delete, upload_status]
    )

    create_project_btn.click(
        create_project,
        inputs=[new_project_input],
        outputs=[upload_status, project_selector, new_project_input]
    )

    process_btn.click(
        process_files, 
        inputs=[file_upload, project_selector], 
        outputs=[upload_status, indexed_files_display, file_to_delete]
    )

    delete_btn.click(
        delete_file,
        inputs=[file_to_delete, project_selector],
        outputs=[upload_status, indexed_files_display, file_to_delete]
    )
    
    msg.submit(chat_response, [msg, chatbot, model_selector, rag_toggle, project_selector], [msg, chatbot])
    submit_btn.click(chat_response, [msg, chatbot, model_selector, rag_toggle, project_selector], [msg, chatbot])
    clear_btn.click(clear_history, outputs=chatbot)

if __name__ == "__main__":
    # å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )